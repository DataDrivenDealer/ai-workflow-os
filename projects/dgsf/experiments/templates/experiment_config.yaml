# Experiment Configuration Template
# DGSF Research Project
# Version: 1.0.0

# ============================================================
# EXPERIMENT METADATA
# ============================================================
experiment:
  # Unique experiment identifier (format: EXP_<TYPE>_<SEQ>)
  id: "${EXPERIMENT_ID}"
  
  # Human-readable experiment name
  name: "${EXPERIMENT_NAME}"
  
  # Semantic version
  version: "0.1.0"
  
  # Creation timestamp
  created: "${TIMESTAMP}"
  
  # Experiment status: draft | running | completed | failed
  status: "draft"
  
  # Research hypothesis being tested
  hypothesis: "${HYPOTHESIS}"
  
  # Priority: P0 (critical) | P1 (high) | P2 (medium) | P3 (low)
  priority: "P1"
  
  # Tags for categorization
  tags:
    - "${TAG1}"
    - "${TAG2}"

# ============================================================
# MODEL CONFIGURATION
# ============================================================
model:
  # Architecture type: deep_sdf | temporal_panel_tree | baseline
  architecture: "${ARCHITECTURE}"
  
  # Encoder configuration
  encoder:
    type: "panel_tree"
    hidden_dim: 128
    num_trees: 8
    max_depth: 6
  
  # Network layers (if applicable)
  network:
    layers: [128, 64, 32]
    activation: "relu"
    dropout: 0.1
    batch_norm: true
  
  # SDF estimator configuration
  sdf_estimator:
    type: "generative"
    output_dim: 1
    constraints:
      - "euler_equation"
      - "no_arbitrage"
  
  # Model-specific hyperparameters
  hyperparameters:
    # Add custom hyperparameters here
    custom_param_1: value

# ============================================================
# TRAINING CONFIGURATION
# ============================================================
training:
  # Optimizer settings
  optimizer: "adam"
  learning_rate: 1.0e-4
  weight_decay: 1.0e-5
  
  # Learning rate scheduler
  scheduler:
    type: "cosine_annealing"
    T_max: 100
    eta_min: 1.0e-6
  
  # Batch settings
  batch_size: 256
  gradient_accumulation: 1
  
  # Training iterations
  epochs: 100
  steps_per_epoch: null  # Auto-calculate from data
  
  # Early stopping
  early_stopping:
    enabled: true
    patience: 10
    metric: "sharpe_ratio"
    mode: "max"  # max | min
    min_delta: 0.001
  
  # Checkpointing
  checkpoint:
    save_best: true
    save_last: true
    save_every_n_epochs: 10
  
  # Mixed precision training
  mixed_precision: true
  
  # Gradient clipping
  gradient_clip:
    enabled: true
    max_norm: 1.0

# ============================================================
# DATA CONFIGURATION
# ============================================================
data:
  # Data source
  source: "dgsf_data_loader"
  
  # Time periods
  train_period: ["2015-01-01", "2021-12-31"]
  val_period: ["2022-01-01", "2022-12-31"]
  test_period: ["2023-01-01", "2023-12-31"]
  
  # Feature set
  features:
    set: "standard_94"
    normalize: true
    fill_na: "median"
  
  # Target configuration
  target:
    variable: "excess_return"
    horizon: 1  # months
  
  # Data filtering
  filters:
    min_market_cap: 1.0e8  # 1 äº¿
    min_trading_days: 200
    exclude_st: true
    exclude_new_listing: 90  # days
  
  # Data loader settings
  loader:
    num_workers: 4
    pin_memory: true
    prefetch_factor: 2

# ============================================================
# EVALUATION CONFIGURATION
# ============================================================
evaluation:
  # Metrics to compute
  metrics:
    primary:
      - sharpe_ratio
      - pricing_error
    secondary:
      - alpha
      - beta
      - max_drawdown
      - turnover
      - information_ratio
  
  # Baseline models for comparison
  baselines:
    - id: "A"
      name: "Sorting"
      sharpe: 0.95
    - id: "C"
      name: "P-tree"
      sharpe: 1.52
    - id: "E"
      name: "FF5"
      sharpe: 0.40
    - id: "F"
      name: "NN-based"
      sharpe: 1.35
  
  # Statistical tests
  statistical_tests:
    sharpe_difference:
      method: "ledoit_wolf"
      alpha: 0.05
    spanning_test:
      enabled: true
    bootstrap:
      n_samples: 1000
      confidence_level: 0.95
  
  # Evaluation frequency
  eval_every_n_epochs: 5

# ============================================================
# LOGGING & TRACKING
# ============================================================
logging:
  # Log level: DEBUG | INFO | WARNING | ERROR
  level: "INFO"
  
  # Experiment tracking
  tracker:
    enabled: true
    backend: "local"  # local | mlflow | wandb
    project: "dgsf-experiments"
    
  # TensorBoard logging
  tensorboard:
    enabled: true
    log_dir: "logs/tensorboard"
  
  # Console output
  console:
    progress_bar: true
    print_every_n_steps: 100

# ============================================================
# REPRODUCIBILITY
# ============================================================
reproducibility:
  # Random seed
  seed: 42
  
  # Deterministic operations (may slow down training)
  deterministic: false
  
  # Code version tracking
  code_version:
    git_commit: "${GIT_COMMIT}"
    git_branch: "${GIT_BRANCH}"
  
  # Environment
  environment:
    python_version: "3.10"
    cuda_version: "11.8"
    pytorch_version: "2.0"

# ============================================================
# RESOURCES
# ============================================================
resources:
  # GPU configuration
  gpu:
    enabled: true
    devices: [0]
    memory_limit: null  # GB, null = no limit
  
  # CPU configuration
  cpu:
    num_threads: 8
  
  # Memory
  memory:
    max_memory_mb: 32000

# ============================================================
# OUTPUT CONFIGURATION
# ============================================================
output:
  # Output directory
  dir: "results/${EXPERIMENT_ID}"
  
  # Save artifacts
  artifacts:
    model_weights: true
    predictions: true
    metrics: true
    figures: true
    config: true
  
  # Figure settings
  figures:
    format: "png"
    dpi: 300
    style: "seaborn"
